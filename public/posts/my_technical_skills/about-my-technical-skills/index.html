<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>About my Technical Skills | Jyotirmay Khavasi</title>
<meta name="keywords" content="markdown">
<meta name="description" content="About my Technical proficiency">
<meta name="author" content="Jyotirmay Khavasi">
<link rel="canonical" href="http://localhost:1313/posts/my_technical_skills/about-my-technical-skills/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.4599eadb9eb2ad3d0a8d6827b41a8fda8f2f4af226b63466c09c5fddbc8706b7.css" integrity="sha256-RZnq256yrT0KjWgntBqP2o8vSvImtjRmwJxf3byHBrc=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/my_technical_skills/about-my-technical-skills/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jyotirmay Khavasi (Alt + H)">Jyotirmay Khavasi</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://theory-in-progress.github.io/Jyotirmay_Khavasi.pdf" title="Resume">
                    <span>Resume</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      About my Technical Skills
    </h1>
    <div class="post-description">
      About my Technical proficiency
    </div>
    <div class="post-meta"><span title='2024-05-29 00:00:00 +0000 UTC'>May 29, 2024</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Jyotirmay Khavasi

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#describe-a-machine-learning-project-you-have-worked-on-what-algorithms-and-tools-did-you-use-and-what-was-the-outcome" aria-label="Describe a machine learning project you have worked on. What algorithms and tools did you use, and what was the outcome?">Describe a machine learning project you have worked on. What algorithms and tools did you use, and what was the outcome?</a><ul>
                        
                <li>
                    <a href="#project-overview" aria-label="Project Overview">Project Overview</a></li>
                <li>
                    <a href="#tools-and-technologies" aria-label="Tools and Technologies">Tools and Technologies</a></li>
                <li>
                    <a href="#outcome" aria-label="Outcome">Outcome</a></li></ul>
                </li>
                <li>
                    <a href="#which-programming-languages-are-you-proficient-in-eg-python-r-java-please-provide-examples-of-how-you-have-used-them-in-past-projects" aria-label="Which programming languages are you proficient in (e.g., Python, R, Java)? Please provide examples of how you have used them in past projects.">Which programming languages are you proficient in (e.g., Python, R, Java)? Please provide examples of how you have used them in past projects.</a><ul>
                        
                <li>
                    <a href="#python" aria-label="Python">Python</a></li>
                <li>
                    <a href="#c" aria-label="C&#43;&#43;">C++</a></li>
                <li>
                    <a href="#sql" aria-label="SQL">SQL</a></li>
                <li>
                    <a href="#tools-and-technologies-1" aria-label="Tools and Technologies">Tools and Technologies</a></li>
                <li>
                    <a href="#examples-of-use" aria-label="Examples of Use">Examples of Use</a></li></ul>
                </li>
                <li>
                    <a href="#describe-your-work-experience-in-nlp" aria-label="Describe your work experience in NLP">Describe your work experience in NLP</a><ul>
                        
                <li>
                    <a href="#work-experience-in-natural-language-processing-nlp" aria-label="Work Experience in Natural Language Processing (NLP)">Work Experience in Natural Language Processing (NLP)</a></li>
                <li>
                    <a href="#internship-at-wolters-kluwer" aria-label="Internship at Wolters Kluwer">Internship at Wolters Kluwer</a></li>
                <li>
                    <a href="#google-summer-of-code--pytorch-ignite" aria-label="Google Summer of Code @ PyTorch-Ignite">Google Summer of Code @ PyTorch-Ignite</a></li>
                <li>
                    <a href="#research-internship-at-hcl-technologies" aria-label="Research Internship at HCL Technologies">Research Internship at HCL Technologies</a></li>
                <li>
                    <a href="#technical-skills-and-tools" aria-label="Technical Skills and Tools">Technical Skills and Tools</a></li></ul>
                </li>
                <li>
                    <a href="#describe-your-experience-training-large-language-models-like-bert-gpt-llama-etc" aria-label="Describe your experience training large language models like BERT, GPT, Llama, etc?">Describe your experience training large language models like BERT, GPT, Llama, etc?</a><ul>
                        
                <li>
                    <a href="#experience-training-large-language-models" aria-label="Experience Training Large Language Models">Experience Training Large Language Models</a></li>
                <li>
                    <a href="#fine-tuning-at-wolters-kluwer" aria-label="Fine-Tuning at Wolters Kluwer">Fine-Tuning at Wolters Kluwer</a></li>
                <li>
                    <a href="#fine-tuning-bert-at-hcl-technologies" aria-label="Fine-Tuning BERT at HCL Technologies">Fine-Tuning BERT at HCL Technologies</a></li>
                <li>
                    <a href="#training-techniques-and-methodologies" aria-label="Training Techniques and Methodologies">Training Techniques and Methodologies</a></li>
                <li>
                    <a href="#key-achievements" aria-label="Key Achievements">Key Achievements</a></li></ul>
                </li>
                <li>
                    <a href="#experience-in-prompt-engineering-information-retrieval-and-retrieval-augmented-generation" aria-label="Experience in Prompt Engineering, Information Retrieval, and Retrieval-Augmented Generation">Experience in Prompt Engineering, Information Retrieval, and Retrieval-Augmented Generation</a><ul>
                        
                <li>
                    <a href="#embedding-and-retrieval-with-llama-index-and-langchain" aria-label="Embedding and Retrieval with Llama-Index and Langchain">Embedding and Retrieval with Llama-Index and Langchain</a></li>
                <li>
                    <a href="#experimentation-with-indexers-and-retrieval-methods" aria-label="Experimentation with Indexers and Retrieval Methods">Experimentation with Indexers and Retrieval Methods</a></li>
                <li>
                    <a href="#practical-applications-and-achievements" aria-label="Practical Applications and Achievements">Practical Applications and Achievements</a></li></ul>
                </li>
                <li>
                    <a href="#experience-with-distributed-systems-and-deep-learning" aria-label="Experience with Distributed Systems and Deep Learning?">Experience with Distributed Systems and Deep Learning?</a><ul>
                        
                <li>
                    <a href="#distributed-systems" aria-label="Distributed Systems">Distributed Systems</a><ul>
                        
                <li>
                    <a href="#data-distributed-parallel-ddp-training" aria-label="Data Distributed Parallel (DDP) Training">Data Distributed Parallel (DDP) Training</a></li></ul>
                </li>
                <li>
                    <a href="#deep-learning" aria-label="Deep Learning">Deep Learning</a><ul>
                        
                <li>
                    <a href="#model-training-and-optimization" aria-label="Model Training and Optimization">Model Training and Optimization</a></li>
                <li>
                    <a href="#pytorch-and-tensorflow" aria-label="PyTorch and TensorFlow">PyTorch and TensorFlow</a></li>
                <li>
                    <a href="#distributed-deep-learning" aria-label="Distributed Deep Learning">Distributed Deep Learning</a></li></ul>
                </li>
                <li>
                    <a href="#key-projects-and-achievements" aria-label="Key Projects and Achievements">Key Projects and Achievements</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="describe-a-machine-learning-project-you-have-worked-on-what-algorithms-and-tools-did-you-use-and-what-was-the-outcome">Describe a machine learning project you have worked on. What algorithms and tools did you use, and what was the outcome?<a hidden class="anchor" aria-hidden="true" href="#describe-a-machine-learning-project-you-have-worked-on-what-algorithms-and-tools-did-you-use-and-what-was-the-outcome">#</a></h2>
<p>I have worked on many projects spanning from use cases of LLMs Retrival Augmented Generation to Core Deep Learning and GPU distrubuted training of text, images and documents.</p>
<p>One of the most impactful machine learning projects I worked on was the development of a table detection pipeline using OpenCV for my role at Wolters Kluwer. This project required an in-depth understanding of computer vision techniques and the ability to create a comprehensive pipeline from data preprocessing to deployment.</p>
<h3 id="project-overview">Project Overview<a hidden class="anchor" aria-hidden="true" href="#project-overview">#</a></h3>
<p>The primary goal was to accurately detect and extract tabular data from PDFs, which involved several steps:</p>
<ol>
<li><strong>Data Augmentation and Preprocessing</strong>: I used various image processing techniques to enhance the quality of the input data. This included operations like scaling, rotation, and noise addition to improve the robustness of the model.</li>
<li><strong>Table Detection</strong>: For detecting tables within the document images, I employed OpenCV’s object detection and image segmentation capabilities. This step involved the use of contour extraction and morphological operations to identify the table boundaries accurately.</li>
<li><strong>Entity Extraction</strong>: Once the tables were detected, the next step was to extract the individual cells and their contents. This was achieved by employing bounding boxes and kernels, leveraging coordinate geometry to precisely segment and reconstruct the table’s structure.</li>
</ol>
<h3 id="tools-and-technologies">Tools and Technologies<a hidden class="anchor" aria-hidden="true" href="#tools-and-technologies">#</a></h3>
<ul>
<li><strong>OpenCV</strong>: The core library used for image processing and computer vision tasks.</li>
<li><strong>Python</strong>: The primary programming language for scripting and implementing the pipeline.</li>
<li><strong>Hugging Face Optimum</strong>: Utilized for converting models to ONNX format, which significantly reduced evaluation time.</li>
<li><strong>Distributed Data Parallel (DDP)</strong>: Used for efficient training and evaluation of the models across multiple GPUs, which helped in handling large datasets and minimizing idle time during processing.</li>
</ul>
<h3 id="outcome">Outcome<a hidden class="anchor" aria-hidden="true" href="#outcome">#</a></h3>
<p>The implementation of this pipeline led to a substantial improvement in the accuracy and efficiency of table detection and data extraction processes. Specifically:</p>
<ul>
<li><strong>Performance Improvement</strong>: The optimized models showed a 4% improvement across all relevant metrics.</li>
<li><strong>Efficiency Gains</strong>: By converting models to the ONNX format, I achieved a 40% reduction in evaluation time for clients.</li>
<li><strong>Scalability</strong>: The use of multi-GPU processing allowed for faster training and evaluation, making the solution scalable for larger datasets.</li>
</ul>
<!-- raw HTML omitted -->
<hr>
<h2 id="which-programming-languages-are-you-proficient-in-eg-python-r-java-please-provide-examples-of-how-you-have-used-them-in-past-projects">Which programming languages are you proficient in (e.g., Python, R, Java)? Please provide examples of how you have used them in past projects.<a hidden class="anchor" aria-hidden="true" href="#which-programming-languages-are-you-proficient-in-eg-python-r-java-please-provide-examples-of-how-you-have-used-them-in-past-projects">#</a></h2>
<p>I am proficient in several programming languages, with Python being my primary language. Here&rsquo;s a detailed overview of how I&rsquo;ve utilized Python and other languages in various projects:</p>
<h3 id="python">Python<a hidden class="anchor" aria-hidden="true" href="#python">#</a></h3>
<p><strong>Experience</strong>: Extensive use during internships, open-source contributions, and academic projects.</p>
<ol>
<li>
<p><strong>Google Summer of Code</strong>:</p>
<ul>
<li><strong>Project</strong>: Contributing to the PyTorch-Ignite library.</li>
<li><strong>Details</strong>: I worked on improving deep learning models and developing reinforcement learning models for the OpenCarRacing-Gym environment of OpenAI. This required a deep understanding of Python&rsquo;s data structures, object-oriented programming, and advanced features.</li>
<li><strong>Tasks</strong>:
<ul>
<li>Enhanced CI/CD pipelines using GitHub workflows.</li>
<li>Implemented Docker containerization for easier deployment.</li>
<li>Wrote event filters for the engine and doctests for various metrics.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Wolters Kluwer Internship</strong>:</p>
<ul>
<li><strong>Project</strong>: Optimizing production-deployed models and developing an OpenCV-based table detection pipeline.</li>
<li><strong>Details</strong>: Utilized Python extensively for data augmentation, object detection, image segmentation, and deploying models in production environments.</li>
<li><strong>Tasks</strong>:
<ul>
<li>Converted models to ONNX format using Hugging Face Optimum.</li>
<li>Implemented a multi-GPU processing solution for efficient model training and evaluation.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="c">C++<a hidden class="anchor" aria-hidden="true" href="#c">#</a></h3>
<p><strong>Experience</strong>: Used in systems programming and backend development.</p>
<ol>
<li><strong>Project</strong>: Developing parsers, shells, and loadable kernel modules.
<ul>
<li><strong>Details</strong>: Wrote parsers and shells to improve system interactions and enhance functionality. Developed kernel modules to extend operating system capabilities.</li>
<li><strong>Tasks</strong>:
<ul>
<li>Implemented code generators for efficient compilation processes.</li>
<li>Ensured system stability and performance through rigorous testing and debugging.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="sql">SQL<a hidden class="anchor" aria-hidden="true" href="#sql">#</a></h3>
<p><strong>Experience</strong>: Utilized for data manipulation and database management.</p>
<ol>
<li><strong>Project</strong>: Data Science projects and internships.
<ul>
<li><strong>Details</strong>: Used SQL for querying and managing data in relational databases.</li>
<li><strong>Tasks</strong>:
<ul>
<li>Wrote complex SQL queries for data extraction and analysis.</li>
<li>Managed MySQL databases in conjunction with Python-based applications.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="tools-and-technologies-1">Tools and Technologies<a hidden class="anchor" aria-hidden="true" href="#tools-and-technologies-1">#</a></h3>
<ul>
<li><strong>Git</strong>: Proficient in using version control systems for collaborative development.</li>
<li><strong>Docker</strong>: Experienced in containerizing applications to ensure consistency across environments.</li>
<li><strong>OpenCV</strong>: Utilized for computer vision tasks, such as image processing and object detection.</li>
<li><strong>PyTorch</strong>: Extensive use for deep learning projects, including model training and evaluation.</li>
<li><strong>Hugging Face Optimum</strong>: Used for model optimization and deployment.</li>
</ul>
<h3 id="examples-of-use">Examples of Use<a hidden class="anchor" aria-hidden="true" href="#examples-of-use">#</a></h3>
<ul>
<li><strong>Python</strong>: Developing reinforcement learning models, optimizing production models, creating data pipelines, and contributing to open-source projects.</li>
<li><strong>C++</strong>: Writing system-level code for parsers, shells, and kernel modules to enhance system functionalities.</li>
<li><strong>Golang</strong>: Building distributed systems that handle concurrent processes efficiently.</li>
<li><strong>SQL</strong>: Extracting and managing data within large-scale data science projects.</li>
</ul>
<p>Overall, my proficiency in these languages has enabled me to tackle a wide range of technical challenges, from low-level system programming to high-level data science and machine learning applications.</p>
<hr>
<h2 id="describe-your-work-experience-in-nlp">Describe your work experience in NLP<a hidden class="anchor" aria-hidden="true" href="#describe-your-work-experience-in-nlp">#</a></h2>
<h3 id="work-experience-in-natural-language-processing-nlp">Work Experience in Natural Language Processing (NLP)<a hidden class="anchor" aria-hidden="true" href="#work-experience-in-natural-language-processing-nlp">#</a></h3>
<p>My experience in Natural Language Processing (NLP) spans multiple projects and roles, where I have leveraged my skills to develop and optimize various NLP models and applications.</p>
<h3 id="internship-at-wolters-kluwer">Internship at Wolters Kluwer<a hidden class="anchor" aria-hidden="true" href="#internship-at-wolters-kluwer">#</a></h3>
<p>During my time as a Data Science Intern at Wolters Kluwer, I worked on optimizing production-deployed models, which included Vision Encoder-Decoder and Layout Transformer models. A significant aspect of this role involved improving the models’ performance on tasks that required accurate text extraction and transformation from complex document structures. I utilized advanced NLP techniques to enhance the models&rsquo; ability to understand and process text within different layouts, ensuring more accurate and efficient information retrieval and processing.</p>
<h3 id="google-summer-of-code--pytorch-ignite">Google Summer of Code @ PyTorch-Ignite<a hidden class="anchor" aria-hidden="true" href="#google-summer-of-code--pytorch-ignite">#</a></h3>
<p>As an open-source contributor during my Google Summer of Code tenure, I contributed to the PyTorch-Ignite library, where I developed templates for Reinforcement Learning using algorithms like DQN and Advantage Actor Critic. Although primarily focused on reinforcement learning, this experience required a deep understanding of NLP techniques for processing and interpreting textual data used in training the models. Additionally, I implemented enhancements to CI/CD pipelines and Docker containerization, which improved the deployment and scalability of NLP models.</p>
<h3 id="research-internship-at-hcl-technologies">Research Internship at HCL Technologies<a hidden class="anchor" aria-hidden="true" href="#research-internship-at-hcl-technologies">#</a></h3>
<p>At HCL Technologies, I worked on a project focused on extracting rules from textual data using NLP. I developed a custom BERT-based architecture for Named Entity Recognition (NER), achieving a recall of 88%. This involved preprocessing textual data, training the model to identify and classify entities accurately, and refining the model to improve its performance. Furthermore, I employed Random Forests for classifying sentences into &lsquo;Rule&rsquo; or &lsquo;Not Rule&rsquo; categories, achieving a 92% accuracy. I also created custom parse trees to convert the recognized rules into logical mathematical expressions, demonstrating my ability to handle complex NLP tasks and integrate them into larger systems.</p>
<h3 id="technical-skills-and-tools">Technical Skills and Tools<a hidden class="anchor" aria-hidden="true" href="#technical-skills-and-tools">#</a></h3>
<p>Throughout these experiences, I have developed proficiency in using various NLP tools and libraries, including:</p>
<ul>
<li><strong>BERT</strong>: For tasks like named entity recognition and text classification.</li>
<li><strong>Hugging Face Transformers</strong>: For leveraging pre-trained models and fine-tuning them for specific NLP tasks.</li>
<li><strong>Python</strong>: Extensive use of Python for scripting, model development, and integration.</li>
<li><strong>PyTorch</strong>: For building and training deep learning models.</li>
<li><strong>OpenCV</strong>: For combining NLP with computer vision tasks, particularly in document processing applications.</li>
</ul>
<!-- raw HTML omitted -->
<hr>
<h2 id="describe-your-experience-training-large-language-models-like-bert-gpt-llama-etc">Describe your experience training large language models like BERT, GPT, Llama, etc?<a hidden class="anchor" aria-hidden="true" href="#describe-your-experience-training-large-language-models-like-bert-gpt-llama-etc">#</a></h2>
<h3 id="experience-training-large-language-models">Experience Training Large Language Models<a hidden class="anchor" aria-hidden="true" href="#experience-training-large-language-models">#</a></h3>
<p>I have extensive experience working with large language models (LLMs) such as BERT, GPT-2, and LLaMA, focusing on fine-tuning these models for specific applications in classification tasks. My work has involved training these models on custom datasets for multi-class and multi-label classification, leveraging advanced techniques to optimize performance and efficiency.</p>
<h3 id="fine-tuning-at-wolters-kluwer">Fine-Tuning at Wolters Kluwer<a hidden class="anchor" aria-hidden="true" href="#fine-tuning-at-wolters-kluwer">#</a></h3>
<p>During my tenure as a Data Science Intern at Wolters Kluwer, I was involved in fine-tuning large language models to enhance their performance on various NLP tasks. One of my key responsibilities was optimizing production-deployed models, including Vision Encoder-Decoder and Layout Transformer models. I fine-tuned these models using custom datasets to improve their accuracy in text extraction and classification tasks. By employing techniques such as data augmentation and batch classification, I was able to achieve significant improvements in model performance metrics.</p>
<h3 id="fine-tuning-bert-at-hcl-technologies">Fine-Tuning BERT at HCL Technologies<a hidden class="anchor" aria-hidden="true" href="#fine-tuning-bert-at-hcl-technologies">#</a></h3>
<p>As a Research Intern at HCL Technologies, I worked extensively with BERT for Named Entity Recognition (NER) and text classification tasks. I fine-tuned BERT on custom datasets to improve its ability to accurately identify and classify entities within textual data. This involved preprocessing the data, setting up the training environment, and using advanced techniques to enhance model accuracy. I achieved a recall of 88% for NER tasks and employed Random Forests alongside BERT to classify sentences into &lsquo;Rule&rsquo; or &lsquo;Not Rule&rsquo; categories with a 92% accuracy rate.</p>
<h3 id="training-techniques-and-methodologies">Training Techniques and Methodologies<a hidden class="anchor" aria-hidden="true" href="#training-techniques-and-methodologies">#</a></h3>
<p>In addition to fine-tuning specific models, I have implemented data distributed parallel training methods to train large language models on GPUs. This approach involves distributing the data and model computations across multiple GPUs, significantly accelerating the training process and allowing for the handling of large datasets. Some of the techniques and tools I have used include:</p>
<ul>
<li><strong>Data Distributed Parallel (DDP) Training</strong>: Implemented synchronized multi-GPU processing to minimize idle time and maximize efficiency during training.</li>
<li><strong>Hugging Face Transformers</strong>: Leveraged pre-trained models from the Hugging Face library and fine-tuned them on domain-specific data.</li>
<li><strong>PyTorch</strong>: Used PyTorch as the primary framework for developing and training models, benefiting from its flexibility and robustness.</li>
<li><strong>ONNX</strong>: Converted models to ONNX format to optimize and reduce evaluation time, ensuring quicker inference times for deployed models.</li>
</ul>
<h3 id="key-achievements">Key Achievements<a hidden class="anchor" aria-hidden="true" href="#key-achievements">#</a></h3>
<ul>
<li><strong>Wolters Kluwer</strong>: Achieved a 4% improvement across all metrics for production-deployed models through fine-tuning and optimization techniques.</li>
<li><strong>HCL Technologies</strong>: Enhanced BERT&rsquo;s NER capabilities to achieve an 88% recall and implemented a hybrid model combining BERT and Random Forests for sentence classification, achieving 92% accuracy.</li>
</ul>
<p>Overall, my experience with large language models encompasses a thorough understanding of model fine-tuning, distributed training methodologies, and practical implementation to solve complex NLP problems effectively.</p>
<p>I have extensive experience working with large language models (LLMs) such as BERT, and GPT-2, focusing on fine-tuning these models for specific applications in classification tasks. My work has involved training these models on custom datasets for multi-class and multi-label classification, leveraging advanced techniques to optimize performance and efficiency.</p>
<p>As a Data Science Intern at Wolters Kluwer, I was involved in fine-tuning large language models to enhance their performance on various NLP tasks. I was involved in fine-tuning large language models to enhance their performance on various NLP tasks. My primary project revolved around training a layout model based on document understanding, utilizing a Swin Transformer. This model processes language tokens and classifies them into different classes, leveraging OCR (Optical Character Recognition) for initial data extraction. The OCR step was crucial in converting scanned documents into machine-readable text, which then served as input for the Swin Transformer model.</p>
<p>I also worked on the classification of large language models such as BERT and GPT-2. I fine-tuned these models on custom datasets for multi-class and multi-label classification tasks, ensuring they could accurately handle domain-specific language and classification challenges. This process involved meticulous data preparation, model tuning, and performance evaluation to achieve the desired accuracy and robustness.</p>
<p>As a Research Intern at HCL Technologies, I worked extensively with BERT for Named Entity Recognition (NER) and text classification tasks. I fine-tuned BERT on custom datasets to improve its ability to accurately identify and classify entities within textual data. I achieved a recall of 88% for NER tasks and employed Random Forests alongside BERT to classify sentences into &lsquo;Rule&rsquo; or &lsquo;Not Rule&rsquo; categories with a 92% accuracy rate.</p>
<p>In addition to fine-tuning specific models, I have implemented data distributed parallel training methods to train large language models on GPUs. This approach involves distributing the data and model computations across multiple GPUs, significantly accelerating the training process and allowing for the handling of large datasets. Some of the techniques and tools I have used include:</p>
<ul>
<li>Data Distributed Parallel (DDP) Training: Implemented synchronized multi-GPU processing to minimize idle time and maximize efficiency during training.</li>
<li>Hugging Face Transformers: Leveraged pre-trained models from the Hugging Face library and fine-tuned them on domain-specific data.</li>
<li>PyTorch: Used PyTorch as the primary framework for developing and training models, benefiting from its flexibility and robustness.</li>
<li>ONNX: Converted models to ONNX format to optimize and reduce evaluation time, ensuring quicker inference times for deployed models.</li>
</ul>
<p>My experience with large language models involves a thorough understanding of model fine-tuning, distributed training methodologies, and practical implementation to solve complex NLP problems effectively.</p>
<hr>
<h2 id="experience-in-prompt-engineering-information-retrieval-and-retrieval-augmented-generation">Experience in Prompt Engineering, Information Retrieval, and Retrieval-Augmented Generation<a hidden class="anchor" aria-hidden="true" href="#experience-in-prompt-engineering-information-retrieval-and-retrieval-augmented-generation">#</a></h2>
<p>I have substantial experience working with prompt engineering and retrieval-augmented generation (RAG) using tools like Llama-Index and Langchain. My work in these areas involves embedding data, efficient storage and retrieval, and enhancing the performance of large language models (LLMs) through context-based query resolution.</p>
<h3 id="embedding-and-retrieval-with-llama-index-and-langchain">Embedding and Retrieval with Llama-Index and Langchain<a hidden class="anchor" aria-hidden="true" href="#embedding-and-retrieval-with-llama-index-and-langchain">#</a></h3>
<p>In my projects, I have embedded data from various sources, such as PDFs, and stored the embedded data in a vector store. This allows for efficient information retrieval based on user queries. By embedding user queries, I can retrieve the top-matching results from the vector store, providing relevant context for the LLM to generate accurate responses. This method ensures that the responses are not only precise but also include proper citations of the sources, enhancing the reliability and traceability of the information provided.</p>
<h3 id="experimentation-with-indexers-and-retrieval-methods">Experimentation with Indexers and Retrieval Methods<a hidden class="anchor" aria-hidden="true" href="#experimentation-with-indexers-and-retrieval-methods">#</a></h3>
<p>To optimize the embedding and retrieval processes, I have experimented with different indexers for indexing data and various retrieval methods for fetching the most relevant information. This experimentation helps in fine-tuning the performance of the system, ensuring that the most relevant context is provided to the LLM for generating answers. My approach typically involves:</p>
<ul>
<li><strong>Indexing</strong>: Utilizing different indexing techniques to efficiently organize and store embedded data.</li>
<li><strong>Retrieval</strong>: Testing various retrieval algorithms to ensure the highest accuracy in matching user queries with the stored data.</li>
<li><strong>Prompt Engineering</strong>: Crafting and refining prompts to ensure that the LLM can generate contextually accurate and relevant responses.</li>
</ul>
<h3 id="practical-applications-and-achievements">Practical Applications and Achievements<a hidden class="anchor" aria-hidden="true" href="#practical-applications-and-achievements">#</a></h3>
<p>By leveraging Llama-Index and Langchain, I have successfully implemented systems that can handle complex information retrieval tasks and augment the capabilities of large language models. These systems are designed to provide detailed, accurate answers with appropriate citations, making them valuable for various applications such as academic research, customer support, and automated documentation.</p>
<p>In summary, my work with prompt engineering and retrieval-augmented generation has enabled me to build sophisticated systems that enhance the performance and accuracy of large language models. By embedding data, optimizing retrieval methods, and refining prompts, I ensure that the LLMs can deliver precise and reliable information tailored to user queries.</p>
<hr>
<h2 id="experience-with-distributed-systems-and-deep-learning">Experience with Distributed Systems and Deep Learning?<a hidden class="anchor" aria-hidden="true" href="#experience-with-distributed-systems-and-deep-learning">#</a></h2>
<p>I have a robust background in both distributed systems and deep learning, leveraging these technologies to develop, train, and deploy sophisticated machine learning models at scale. My experience spans various aspects of these fields, from optimizing model training to implementing efficient data processing pipelines.</p>
<h3 id="distributed-systems">Distributed Systems<a hidden class="anchor" aria-hidden="true" href="#distributed-systems">#</a></h3>
<h4 id="data-distributed-parallel-ddp-training">Data Distributed Parallel (DDP) Training<a hidden class="anchor" aria-hidden="true" href="#data-distributed-parallel-ddp-training">#</a></h4>
<p>In my role at Wolters Kluwer, I implemented Data Distributed Parallel (DDP) training to optimize the performance of large language models. By distributing the training process across multiple GPUs, I significantly reduced training time and enhanced the scalability of the models. This involved synchronizing the workloads across GPUs, minimizing idle times, and ensuring efficient utilization of hardware resources.</p>
<h3 id="deep-learning">Deep Learning<a hidden class="anchor" aria-hidden="true" href="#deep-learning">#</a></h3>
<h4 id="model-training-and-optimization">Model Training and Optimization<a hidden class="anchor" aria-hidden="true" href="#model-training-and-optimization">#</a></h4>
<p>My deep learning experience includes training various models such as BERT, GPT-2, and custom neural networks. I have fine-tuned these models for specific tasks, such as text classification, named entity recognition, and image processing. This work often involved hyperparameter tuning, data augmentation, and implementing advanced training techniques to improve model accuracy and robustness.</p>
<h4 id="pytorch-and-tensorflow">PyTorch and TensorFlow<a hidden class="anchor" aria-hidden="true" href="#pytorch-and-tensorflow">#</a></h4>
<p>I am proficient in using deep learning frameworks like PyTorch and TensorFlow. These tools are essential for developing and training neural networks, and I have utilized them to build and deploy models for various applications. My projects often involve customizing network architectures, optimizing training loops, and integrating these models into larger systems.</p>
<h4 id="distributed-deep-learning">Distributed Deep Learning<a hidden class="anchor" aria-hidden="true" href="#distributed-deep-learning">#</a></h4>
<p>Combining my knowledge of distributed systems and deep learning, I have implemented distributed deep learning solutions to handle large-scale training tasks. By parallelizing the training process and distributing the data across multiple nodes, I could train models more efficiently and effectively handle large datasets. This approach ensures that deep learning models can be trained quickly without compromising accuracy or performance.</p>
<h3 id="key-projects-and-achievements">Key Projects and Achievements<a hidden class="anchor" aria-hidden="true" href="#key-projects-and-achievements">#</a></h3>
<ul>
<li><strong>Table Detection Pipeline</strong>: Developed a table detection pipeline using OpenCV for document understanding tasks. This involved detecting tables in scanned documents, extracting tabular data using geometric algorithms, and training a model to improve detection accuracy.</li>
<li><strong>Vision Encoder-Decoder Models</strong>: At Wolters Kluwer, I optimized vision encoder-decoder models for document processing tasks. This included converting models to the ONNX format to reduce evaluation time and implementing synchronized multi-GPU processing for efficient model training.</li>
<li><strong>Reinforcement Learning</strong>: As part of the Google Summer of Code, I developed reinforcement learning models using DQN and Advantage Actor Critic algorithms. This project involved parallel processing for environment simulation and optimizing reinforcement learning workflows.</li>
</ul>
<!-- raw HTML omitted -->


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/markdown/">Markdown</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on x"
            href="https://x.com/intent/tweet/?text=About%20my%20Technical%20Skills&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f&amp;hashtags=markdown">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f&amp;title=About%20my%20Technical%20Skills&amp;summary=About%20my%20Technical%20Skills&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f&title=About%20my%20Technical%20Skills">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on whatsapp"
            href="https://api.whatsapp.com/send?text=About%20my%20Technical%20Skills%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on telegram"
            href="https://telegram.me/share/url?text=About%20my%20Technical%20Skills&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share About my Technical Skills on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=About%20my%20Technical%20Skills&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_technical_skills%2fabout-my-technical-skills%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Jyotirmay Khavasi</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
